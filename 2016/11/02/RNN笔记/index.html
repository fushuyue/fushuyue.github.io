<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>RNN笔记 | 长海冰茶火</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">RNN笔记</h1><a id="logo" href="/.">长海冰茶火</a><p class="description">很惭愧，做了一点微小的工作和笔记</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">RNN笔记</h1><div class="post-meta">Nov 2, 2016<span> | </span><span class="category"><a href="/categories/深度学习/">深度学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/11/02/RNN笔记/" href="/2016/11/02/RNN笔记/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Main-idea"><span class="toc-number">1.</span> <span class="toc-text">Main idea</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-problem-of-RNN"><span class="toc-number">2.</span> <span class="toc-text">The problem of RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Solving-the-problem"><span class="toc-number">3.</span> <span class="toc-text">Solving the problem</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Leaky-units"><span class="toc-number">3.1.</span> <span class="toc-text">Leaky units</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LSTM-and-other-gated-RNNs"><span class="toc-number">3.2.</span> <span class="toc-text">LSTM and other gated RNNs</span></a></li></ol></li></ol></div></div><div class="post-content"><p>Recurrent neural networks are networks with loops in them, allowing information to persist, to allows information to be passed from one step of the network to the next.</p>
<h3 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea"></a>Main idea</h3><p><code>Parameter sharing</code> makes it possible to extend and apply the model to examples of different forms<br>(different lengths, here) and generalize across them.</p>
<p>Such sharing is particularly important when a specific piece of information can occur <code>at multiple positions</code> within the sequence.</p>
<p>The parameter sharing used in recurrent networks relies on the assumption that the same parameters can be used for different time steps.<br>Equivalently, the assumption is that the conditional probability distribution over the variables at time t+1 given the variables at time t is <code>stationary</code>, meaning that the relationship between the previous time step and the next time step does not depend on t. </p>
<h3 id="The-problem-of-RNN"><a href="#The-problem-of-RNN" class="headerlink" title="The problem of RNN"></a>The problem of RNN</h3><p>In a traditional recurrent neural network, <code>the magnitude of weights</code> in the transition matrix can have a strong impact on the learning process.</p>
<p>The basic problem is that gradients propagated over many stages tend to either <code>vanish</code> (most of the time) or <code>explode</code>(rarely, but with much damage to the optimization). </p>
<p>If the weights in this matrix are small, it can lead to a situation called vanishing gradients where the gradient signal gets so small that learning either becomes very slow or stops working altogether. </p>
<p>Conversely, if the weights in this matrix are large, it can lead to a situation where the gradient signal is so large that it can cause learning to diverge. This is often referred to as exploding gradients.</p>
<p>In short: </p>
<ul>
<li>if w is small, memory decay exponentially</li>
<li>if w is large, gradients explode, training will be highly unstable</li>
</ul>
<h3 id="Solving-the-problem"><a href="#Solving-the-problem" class="headerlink" title="Solving the problem"></a>Solving the problem</h3><p>Core idea: create paths throught time that have derivatives that neither vanish nor explode</p>
<h4 id="Leaky-units"><a href="#Leaky-units" class="headerlink" title="Leaky units"></a>Leaky units</h4><ol>
<li>add skipping units</li>
<li>use leaky units</li>
</ol>
<h4 id="LSTM-and-other-gated-RNNs"><a href="#LSTM-and-other-gated-RNNs" class="headerlink" title="LSTM and other gated RNNs"></a>LSTM and other gated RNNs</h4><p>LSTM model introduces a new structure called a memory cell which is composed of four cells:<br><img src="http://deeplearning.net/tutorial/_images/lstm_memorycell.png" alt=""></p>
<ol>
<li><p>input gate</p>
<ul>
<li>The input gate can allow incoming signal to alter the state of the memory cell or block it</li>
</ul>
</li>
<li><p>forget gate</p>
<ul>
<li>The forget gate can modulate the memory cell’s self-recurrent connection, allowing the cell to remember or forget its previous state, as needed.</li>
</ul>
</li>
<li><p>output gate</p>
<ul>
<li>the output gate can allow the state of the memory cell to have an effect on other neurons or prevent it</li>
</ul>
</li>
<li><p>state units</p>
</li>
</ol>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2016/11/02/RNN笔记/" data-id="civix2ad30000et61i7dglh4s" class="article-share-link">分享到</a><div class="tags"><a href="/tags/RNN/">RNN</a></div><div class="post-nav"><a href="/2016/10/26/量化策略评价体系/" class="next">量化策略评价体系</a></div><div data-thread-key="2016/11/02/RNN笔记/" data-title="RNN笔记" data-url="http://yoursite.com/2016/11/02/RNN笔记/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/11/02/RNN笔记/" data-title="RNN笔记" data-url="http://yoursite.com/2016/11/02/RNN笔记/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/python爬虫/">python爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/期权/">期权</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/量化交易/">量化交易</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/urllib/" style="font-size: 15px;">urllib</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/threading/" style="font-size: 15px;">threading</a> <a href="/tags/multiprocessing/" style="font-size: 15px;">multiprocessing</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/quant/" style="font-size: 15px;">quant</a> <a href="/tags/cnn/" style="font-size: 15px;">cnn</a> <a href="/tags/cuda/" style="font-size: 15px;">cuda</a> <a href="/tags/择时/" style="font-size: 15px;">择时</a> <a href="/tags/RL/" style="font-size: 15px;">RL</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/11/02/RNN笔记/">RNN笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/26/量化策略评价体系/">量化策略评价体系</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/26/scrapy库学习/">scrapy库学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/26/择时体系梳理/">择时体系梳理</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/22/requests库/">requests库</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/22/urllib库学习/">urllib库学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/21/深度强化学习笔记/">深度强化学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/19/复权问题处理/">复权问题处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/17/python多进程编程/">python多进程编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/16/python_With_statement/">With statement in python</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 关于我</i></div><ul></ul><a href="http://www.github.com/fushuyue" title="Github" target="_blank">Github</a><ul></ul><a href="https://www.linkedin.com/in/fushuyue" title="Linkedin" target="_blank">Linkedin</a><ul></ul><a href="sfu11@illinois.edu" title="Email" target="_blank">Email</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">长海冰茶火.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'fushuyue'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>